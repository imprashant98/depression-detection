{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/prashantkarna/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "nltk.download('vader_lexicon')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/prashantkarna/Documents/Research Materials/Datasets/depression_dataset_reddit_cleaned.csv')\n",
    "# rename the columns\n",
    "data = data.rename(columns={'clean_text': 'text', 'is_depression': 'class'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7731, 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# data_split = np.array_split(data, 3)\n",
    "# data = data_split[0]\n",
    "# data = data.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to r depression s check in post a plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i ve kind of stuffed around a lot in my life d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sleep is my greatest and most comforting escap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i m year old turning soon in a few month i liv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i live alone and despite me being prone to lon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i m not looking for sympathy just simply to st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i don t know how to communicate all of my thou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mom i m sad it hurt in my heart the feeling fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0  we understand that most people who reply immed...      1\n",
       "1  welcome to r depression s check in post a plac...      1\n",
       "2  anyone else instead of sleeping more when depr...      1\n",
       "3  i ve kind of stuffed around a lot in my life d...      1\n",
       "4  sleep is my greatest and most comforting escap...      1\n",
       "5  i m year old turning soon in a few month i liv...      1\n",
       "6  i live alone and despite me being prone to lon...      1\n",
       "7  i m not looking for sympathy just simply to st...      1\n",
       "8  i don t know how to communicate all of my thou...      1\n",
       "9  mom i m sad it hurt in my heart the feeling fa...      1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashantkarna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove emails\n",
    "email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    # removing all special charachter\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(X['text'][i]))\n",
    "    # make document as lowerCase\n",
    "    review = review.lower()\n",
    "    # splitting the documents into words for ex ['iam', 'omar']\n",
    "    review = review.split()\n",
    "    # make limmatization --> (change, changing, changes)---> (change)\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords)]\n",
    "    # join the document agian\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    # removing mails\n",
    "    for r in regexes_to_remove:\n",
    "        X['text'][i] = re.sub(r, '', review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3900\n",
      "1    3831\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of instances in each class\n",
    "class_counts = data['class'].value_counts()\n",
    "\n",
    "# Print the class counts\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# # balance the dataset\n",
    "# df_majority = data[data['class'] == '0']\n",
    "# df_minority = data[data['class'] == '1']\n",
    "# df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=123)\n",
    "# data = pd.concat([df_majority, df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3900\n",
      "1    3831\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of instances in each class\n",
    "class_counts = data['class'].value_counts()\n",
    "\n",
    "# Print the class counts\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_vectorizer_uni = TfidfVectorizer(max_features=10000,ngram_range=(1,1))\n",
    "tfidf_vectorizer_bi = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "tfidf_vectorizer_tri = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X['text'])\n",
    "# X_tfidf_test = tfidf_vectorizer.transform(X['text'])\n",
    "\n",
    "X_tfidf_uni = tfidf_vectorizer_uni.fit_transform(X['text'])\n",
    "# X_tfidf_test_uni = tfidf_vectorizer_uni.transform(X['text'])\n",
    "\n",
    "X_tfidf_bi = tfidf_vectorizer_bi.fit_transform(X['text'])\n",
    "# X_tfidf_test_bi =tfidf_vectorizer_bi.transform(X'text'])\n",
    "\n",
    "X_tfidf_tri = tfidf_vectorizer_tri.fit_transform(X['text'])\n",
    "# X_tfidf_test_tri = tfidf_vectorizer_tri.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Feature selection using ExtraTreesClassifier and SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(ExtraTreesClassifier(n_estimators=300, random_state=42))\n",
    "\n",
    "                                        \n",
    "# Unigram feature selection\n",
    "selector_tfidf= selector.fit(X_tfidf, y)\n",
    "X_selection = selector_tfidf.transform(X_tfidf)\n",
    "\n",
    "\n",
    "# Unigram feature selection\n",
    "selector_uni= selector.fit(X_tfidf_uni, y)\n",
    "X_selection_uni = selector_uni.transform(X_tfidf_uni)\n",
    "\n",
    "# Bigram feature selection\n",
    "selector_bi = selector.fit(X_tfidf_bi, y)\n",
    "X_selection_bi = selector_bi.transform(X_tfidf_bi)\n",
    "\n",
    "# Trigram feature selection\n",
    "selector_tri= selector.fit(X_tfidf_tri, y)\n",
    "X_selection_tri = selector_tri.transform(X_tfidf_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to csv\n",
    "# X_df = pd.DataFrame(X_selection.todense())\n",
    "# X_df.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to cs\n",
    "# X_df_uni = pd.DataFrame(X_selection.todense())\n",
    "# X_df_uni.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_uni_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to csv\n",
    "# X_df_bi = pd.DataFrame(X_selection.todense())\n",
    "# X_df_bi.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_bi_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to csv\n",
    "# X_df_tri = pd.DataFrame(X_selection.todense())\n",
    "# X_df_tri.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_tri_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_tri_dataset.csv')\n",
    "# data1.columns\n",
    "\n",
    "# # X = data1.drop('class', axis=1)\n",
    "# # y = data1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       780\n",
      "           1       0.98      0.93      0.95       767\n",
      "\n",
      "    accuracy                           0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95       780\n",
      "           1       0.96      0.93      0.95       767\n",
      "\n",
      "    accuracy                           0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90       780\n",
      "           1       0.86      0.97      0.91       767\n",
      "\n",
      "    accuracy                           0.91      1547\n",
      "   macro avg       0.91      0.91      0.91      1547\n",
      "weighted avg       0.91      0.91      0.91      1547\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       780\n",
      "           1       0.98      0.96      0.97       766\n",
      "\n",
      "    accuracy                           0.97      1546\n",
      "   macro avg       0.97      0.97      0.97      1546\n",
      "weighted avg       0.97      0.97      0.97      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       780\n",
      "           1       0.96      0.96      0.96       766\n",
      "\n",
      "    accuracy                           0.96      1546\n",
      "   macro avg       0.96      0.96      0.96      1546\n",
      "weighted avg       0.96      0.96      0.96      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90       780\n",
      "           1       0.86      0.98      0.91       766\n",
      "\n",
      "    accuracy                           0.91      1546\n",
      "   macro avg       0.92      0.91      0.91      1546\n",
      "weighted avg       0.92      0.91      0.91      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       780\n",
      "           1       0.98      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       780\n",
      "           1       0.96      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       780\n",
      "           1       0.84      0.96      0.90       766\n",
      "\n",
      "    accuracy                           0.89      1546\n",
      "   macro avg       0.90      0.89      0.89      1546\n",
      "weighted avg       0.90      0.89      0.89      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       780\n",
      "           1       0.97      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       780\n",
      "           1       0.95      0.93      0.94       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.82      0.89       780\n",
      "           1       0.84      0.97      0.90       766\n",
      "\n",
      "    accuracy                           0.90      1546\n",
      "   macro avg       0.91      0.90      0.90      1546\n",
      "weighted avg       0.91      0.90      0.90      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       780\n",
      "           1       0.98      0.96      0.97       766\n",
      "\n",
      "    accuracy                           0.97      1546\n",
      "   macro avg       0.97      0.97      0.97      1546\n",
      "weighted avg       0.97      0.97      0.97      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       780\n",
      "           1       0.96      0.96      0.96       766\n",
      "\n",
      "    accuracy                           0.96      1546\n",
      "   macro avg       0.96      0.96      0.96      1546\n",
      "weighted avg       0.96      0.96      0.96      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92       780\n",
      "           1       0.88      0.98      0.93       766\n",
      "\n",
      "    accuracy                           0.92      1546\n",
      "   macro avg       0.93      0.92      0.92      1546\n",
      "weighted avg       0.93      0.92      0.92      1546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression(C=500, penalty='l1', solver='saga')\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_bi, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_bi[train_index], X_selection_bi[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "# # Print best accuracy score for each classifier\n",
    "# print(\"LinearSVC best accuracy:\", max(svm_acc))\n",
    "# print(\"Logistic Regression best accuracy:\", max(lr_acc))\n",
    "# print(\"Naive Bayes best accuracy:\", max(nb_acc))\n",
    "# # print(\"XGBoost best accuracy:\", max(xgb_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       780\n",
      "           1       0.98      0.93      0.95       767\n",
      "\n",
      "    accuracy                           0.95      1547\n",
      "   macro avg       0.96      0.95      0.95      1547\n",
      "weighted avg       0.96      0.95      0.95      1547\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       780\n",
      "           1       0.98      0.92      0.95       767\n",
      "\n",
      "    accuracy                           0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.90       780\n",
      "           1       0.87      0.94      0.90       767\n",
      "\n",
      "    accuracy                           0.90      1547\n",
      "   macro avg       0.90      0.90      0.90      1547\n",
      "weighted avg       0.90      0.90      0.90      1547\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       780\n",
      "           1       0.97      0.96      0.97       766\n",
      "\n",
      "    accuracy                           0.97      1546\n",
      "   macro avg       0.97      0.97      0.97      1546\n",
      "weighted avg       0.97      0.97      0.97      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       780\n",
      "           1       0.98      0.92      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89       780\n",
      "           1       0.86      0.96      0.90       766\n",
      "\n",
      "    accuracy                           0.90      1546\n",
      "   macro avg       0.90      0.90      0.90      1546\n",
      "weighted avg       0.90      0.90      0.90      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       780\n",
      "           1       0.98      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       780\n",
      "           1       0.98      0.91      0.94       766\n",
      "\n",
      "    accuracy                           0.94      1546\n",
      "   macro avg       0.95      0.94      0.94      1546\n",
      "weighted avg       0.95      0.94      0.94      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.89       780\n",
      "           1       0.86      0.93      0.89       766\n",
      "\n",
      "    accuracy                           0.89      1546\n",
      "   macro avg       0.89      0.89      0.89      1546\n",
      "weighted avg       0.89      0.89      0.89      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       780\n",
      "           1       0.97      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       780\n",
      "           1       0.97      0.91      0.94       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.94      0.94      1546\n",
      "weighted avg       0.95      0.95      0.94      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       780\n",
      "           1       0.84      0.95      0.89       766\n",
      "\n",
      "    accuracy                           0.89      1546\n",
      "   macro avg       0.89      0.89      0.89      1546\n",
      "weighted avg       0.89      0.89      0.89      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       780\n",
      "           1       0.98      0.96      0.97       766\n",
      "\n",
      "    accuracy                           0.97      1546\n",
      "   macro avg       0.97      0.97      0.97      1546\n",
      "weighted avg       0.97      0.97      0.97      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       780\n",
      "           1       0.98      0.94      0.96       766\n",
      "\n",
      "    accuracy                           0.96      1546\n",
      "   macro avg       0.96      0.96      0.96      1546\n",
      "weighted avg       0.96      0.96      0.96      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       780\n",
      "           1       0.87      0.95      0.91       766\n",
      "\n",
      "    accuracy                           0.91      1546\n",
      "   macro avg       0.91      0.91      0.91      1546\n",
      "weighted avg       0.91      0.91      0.91      1546\n",
      "\n",
      "LinearSVC best accuracy: 0.9695989650711514\n",
      "Logistic Regression best accuracy: 0.961836998706339\n",
      "Naive Bayes best accuracy: 0.9055627425614489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC(C=1.0,dual=False, max_iter=1000,random_state=42)\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_tri, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_tri[train_index], X_selection_tri[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "# Print best accuracy score for each classifier\n",
    "print(\"LinearSVC best accuracy:\", max(svm_acc))\n",
    "print(\"Logistic Regression best accuracy:\", max(lr_acc))\n",
    "print(\"Naive Bayes best accuracy:\", max(nb_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       780\n",
      "           1       0.97      0.93      0.95       767\n",
      "\n",
      "    accuracy                           0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       780\n",
      "           1       0.98      0.91      0.94       767\n",
      "\n",
      "    accuracy                           0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       780\n",
      "           1       0.86      0.94      0.90       767\n",
      "\n",
      "    accuracy                           0.89      1547\n",
      "   macro avg       0.90      0.89      0.89      1547\n",
      "weighted avg       0.90      0.89      0.89      1547\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       780\n",
      "           1       0.97      0.95      0.96       766\n",
      "\n",
      "    accuracy                           0.96      1546\n",
      "   macro avg       0.96      0.96      0.96      1546\n",
      "weighted avg       0.96      0.96      0.96      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       780\n",
      "           1       0.97      0.91      0.94       766\n",
      "\n",
      "    accuracy                           0.94      1546\n",
      "   macro avg       0.94      0.94      0.94      1546\n",
      "weighted avg       0.94      0.94      0.94      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       780\n",
      "           1       0.87      0.95      0.91       766\n",
      "\n",
      "    accuracy                           0.90      1546\n",
      "   macro avg       0.91      0.90      0.90      1546\n",
      "weighted avg       0.91      0.90      0.90      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       780\n",
      "           1       0.98      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.96      1546\n",
      "   macro avg       0.96      0.96      0.96      1546\n",
      "weighted avg       0.96      0.96      0.96      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       780\n",
      "           1       0.98      0.91      0.94       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.95      0.95      0.95      1546\n",
      "weighted avg       0.95      0.95      0.95      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88       780\n",
      "           1       0.85      0.94      0.89       766\n",
      "\n",
      "    accuracy                           0.89      1546\n",
      "   macro avg       0.89      0.89      0.89      1546\n",
      "weighted avg       0.89      0.89      0.89      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       780\n",
      "           1       0.97      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.96      0.95      0.95      1546\n",
      "weighted avg       0.96      0.95      0.95      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       780\n",
      "           1       0.97      0.90      0.94       766\n",
      "\n",
      "    accuracy                           0.94      1546\n",
      "   macro avg       0.94      0.94      0.94      1546\n",
      "weighted avg       0.94      0.94      0.94      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.87       780\n",
      "           1       0.84      0.94      0.89       766\n",
      "\n",
      "    accuracy                           0.88      1546\n",
      "   macro avg       0.89      0.88      0.88      1546\n",
      "weighted avg       0.89      0.88      0.88      1546\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       780\n",
      "           1       0.99      0.95      0.97       766\n",
      "\n",
      "    accuracy                           0.97      1546\n",
      "   macro avg       0.97      0.97      0.97      1546\n",
      "weighted avg       0.97      0.97      0.97      1546\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       780\n",
      "           1       0.98      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.96      0.95      0.95      1546\n",
      "weighted avg       0.96      0.95      0.95      1546\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90       780\n",
      "           1       0.86      0.95      0.91       766\n",
      "\n",
      "    accuracy                           0.90      1546\n",
      "   macro avg       0.91      0.90      0.90      1546\n",
      "weighted avg       0.91      0.90      0.90      1546\n",
      "\n",
      "LinearSVC best accuracy: 0.9689521345407504\n",
      "Logistic Regression best accuracy: 0.9540750323415266\n",
      "Naive Bayes best accuracy: 0.9042690815006468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_uni, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_uni[train_index], X_selection_uni[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "# Print best accuracy score for each classifier\n",
    "print(\"LinearSVC best accuracy:\", max(svm_acc))\n",
    "print(\"Logistic Regression best accuracy:\", max(lr_acc))\n",
    "print(\"Naive Bayes best accuracy:\", max(nb_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier best accuracy: 0.964424320827943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define list to store accuracy scores\n",
    "rf_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_tri, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_tri[train_index], X_selection_tri[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using Random Forest Classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_acc.append(accuracy_score(y_test, rf_pred))\n",
    "\n",
    "# Print best accuracy score for Random Forest Classifier\n",
    "print(\"Random Forest Classifier best accuracy:\", max(rf_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier best accuracy: 0.9702457956015524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define XGBoost Classifier\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define list to store accuracy scores\n",
    "xgb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_tri, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_tri[train_index], X_selection_tri[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "# Print best accuracy score for XGBoost Classifier\n",
    "print(\"XGBoost Classifier best accuracy:\", max(xgb_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
