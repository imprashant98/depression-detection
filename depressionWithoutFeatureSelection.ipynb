{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[nltk_data] Error loading vader_lexicon: <urlopen error [Errno 8]\n",
      "[nltk_data]     nodename nor servname provided, or not known>\n"
=======
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/prashantkarna/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
>>>>>>> d413ecf (Final Commit)
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "nltk.download('vader_lexicon')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "import neattext.functions as nfx\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Dense,LSTM,Bidirectional,GlobalMaxPooling1D,Input,Dropout\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/prashantkarna/Documents/Research Materials/Datasets/Suicide_Detection.csv')\n",
    "\n",
    "data['class'] = data['class'].replace(['suicide'], 1)\n",
    "data['class'] = data['class'].replace(['non-suicide'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data2 = pd.read_csv('/Users/prashantkarna/Documents/Research Materials/Datasets/depression_dataset_reddit_cleaned.csv')\n",
    "# # rename the columns\n",
    "# data2 = data2.rename(columns={'clean_text': 'text', 'is_depression': 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([data1,data2])\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data_split = np.array_split(Suicide, 3)\n",
    "# Suicide = data_split[0]\n",
    "# data = data.drop('Unnamed: 0', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232074, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
=======
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashantkarna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
>>>>>>> d413ecf (Final Commit)
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_arr = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove emails\n",
    "email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    # removing all special charachter\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(X['text'][i]))\n",
    "    # make document as lowerCase\n",
    "    review = review.lower()\n",
    "    # splitting the documents into words for ex ['iam', 'omar']\n",
    "    review = review.split()\n",
    "    # make lemmatization --> (change, changing, changes)---> (change)\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords)]\n",
    "    # join the document agian\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    # removing mails\n",
    "    for r in regexes_to_remove:\n",
    "        X['text'][i] = re.sub(r, '', review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer=Tokenizer()\n",
    "# tokenizer.fit_on_texts(X['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq=pd.DataFrame(tokenizer.word_counts.items(),columns=['word','count']).sort_values(by='count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# sns.barplot(x='count',y='word',data=word_freq.iloc[:50])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names=word_freq['word'].values\n",
    "# wc=WordCloud(max_words=400)\n",
    "# wc.generate(' '.join(word for word in feature_names[500:3500] ))\n",
    "# plt.figure(figsize=(10,15))\n",
    "# plt.axis('off')\n",
    "# plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_uni = TfidfVectorizer(max_features=10000,ngram_range=(1,1))\n",
    "tfidf_vectorizer_bi = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "tfidf_vectorizer_tri = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "X_tfidf_uni = tfidf_vectorizer_uni.fit_transform(X['text'])\n",
    "# X_tfidf_test_uni = tfidf_vectorizer_uni.transform(X_test['text'])\n",
    "\n",
    "X_tfidf_bi = tfidf_vectorizer_bi.fit_transform(X['text'])\n",
    "# X_tfidf_test_bi =tfidf_vectorizer_bi.transform(X_test['text'])\n",
    "\n",
    "X_tfidf_tri = tfidf_vectorizer_tri.fit_transform(X['text'])\n",
    "# X_tfidf_test_tri = tfidf_vectorizer_tri.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "  \n",
    "X_bow = vectorizer.fit_transform(X['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "# def get_vader_scores(data):\n",
    "#     sid=SIA()\n",
    "#     vader_df=data.copy()\n",
    "#     vader_df['scores'] = vader_df['text'].apply(lambda txt: sid.polarity_scores(str(txt)))\n",
    "    \n",
    "#     vader_df['neg_score'] = vader_df['scores'].apply(lambda txt: txt['neg'])\n",
    "#     vader_df['neu_score'] =vader_df['scores'].apply(lambda txt: txt['neu'])\n",
    "#     vader_df['pos_score'] = vader_df['scores'].apply(lambda txt: txt['pos'])\n",
    "#     vader_df['compound'] = vader_df['scores'].apply(lambda txt: txt['compound'])\n",
    "#     vader_df.drop('scores', axis=1, inplace=True)\n",
    "#     vader_df.drop('text', axis=1, inplace=True)\n",
    "#     return vader_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_vader = get_vader_scores(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
=======
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prashantkarna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
>>>>>>> d413ecf (Final Commit)
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text data into sentences and words\n",
    "sentences = [nltk.word_tokenize(doc) for doc in X['text']]\n",
    "\n",
    "# Train a word2vec model on the tokenized text data\n",
    "model = gensim.models.Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=8)\n",
    "model.build_vocab(sentences, update=True)\n",
    "# Save the trained model to disk\n",
    "model.save('/Users/prashantkarna/Documents/Research Materials/final code/pretrained_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "(62998562, 70053935)"
=======
       "(63000608, 70053935)"
>>>>>>> d413ecf (Final Commit)
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# Convert the text data into word2vec vectors\n",
    "X_word2vec = []\n",
    "for doc in X['text']:\n",
    "    doc_vec = []\n",
    "    for word in doc.split():\n",
    "        if word in model.wv.key_to_index:\n",
    "            doc_vec.append(model.wv[word])\n",
    "    if doc_vec:\n",
    "        X_word2vec.append(np.mean(doc_vec, axis=0))\n",
    "    else:\n",
    "        X_word2vec.append(np.zeros(model.vector_size))\n",
    "X_word2vec = np.array(X_word2vec,dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Assuming the target variable is stored in y\n",
    "# le = LabelEncoder()\n",
    "# y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Assuming each feature vector is stored in a separate sparse matrix\n",
    "# scaler_uni = StandardScaler(with_mean=False)\n",
    "# X_tfidf_uni_norm = scaler_uni.fit_transform(X_tfidf_uni)\n",
    "\n",
    "# scaler_bi = StandardScaler(with_mean=False)\n",
    "# X_tfidf_bi_norm = scaler_bi.fit_transform(X_tfidf_bi)\n",
    "\n",
    "# scaler_tri = StandardScaler(with_mean=False)\n",
    "# X_tfidf_tri_norm = scaler_tri.fit_transform(X_tfidf_tri)\n",
    "\n",
    "# scaler_bow = StandardScaler(with_mean=False)\n",
    "# bow_norm = scaler_bow.fit_transform(X_bow)\n",
    "\n",
    "# # scaler_vader = StandardScaler(with_mean=False)\n",
    "# # vader_norm = scaler_vader.fit_transform(X_vader)\n",
    "\n",
    "# scaler_word2vec = StandardScaler(with_mean=False)\n",
    "# word2vec_norm = scaler_word2vec.fit_transform(X_word2vec)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 26,
>>>>>>> d413ecf (Final Commit)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.93     23208\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     23207\n",
      "           1       0.87      0.95      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23207\n",
      "           1       0.89      0.92      0.90     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92     23207\n",
      "           1       0.93      0.89      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.91     23207\n",
      "           1       0.88      0.95      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     23207\n",
      "           1       0.89      0.92      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23207\n",
      "           1       0.93      0.90      0.91     23208\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     23208\n",
      "           1       0.94      0.92      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     23208\n",
      "           1       0.94      0.92      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.94      0.93      0.93     46415\n",
      "weighted avg       0.94      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90     23208\n",
      "           1       0.88      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90     23208\n",
      "           1       0.89      0.92      0.90     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     23208\n",
      "           1       0.93      0.89      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23208\n",
      "           1       0.94      0.93      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23208\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.90     23208\n",
      "           1       0.88      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23208\n",
      "           1       0.89      0.92      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23208\n",
      "           1       0.93      0.89      0.91     23207\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46414\n",
      "   macro avg       0.94      0.94      0.94     46414\n",
      "weighted avg       0.94      0.94      0.94     46414\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46414\n",
      "   macro avg       0.94      0.94      0.94     46414\n",
      "weighted avg       0.94      0.94      0.94     46414\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     23207\n",
      "           1       0.88      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46414\n",
      "   macro avg       0.91      0.91      0.91     46414\n",
      "weighted avg       0.91      0.91      0.91     46414\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23207\n",
      "           1       0.89      0.92      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46414\n",
      "   macro avg       0.91      0.91      0.91     46414\n",
      "weighted avg       0.91      0.91      0.91     46414\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23207\n",
      "           1       0.93      0.90      0.92     23207\n",
      "\n",
      "    accuracy                           0.92     46414\n",
      "   macro avg       0.92      0.92      0.92     46414\n",
      "weighted avg       0.92      0.92      0.92     46414\n",
      "\n",
      "LinearSVC best accuracy: 0.9350982930320371\n",
      "Logistic Regression best accuracy: 0.9366624462661595\n",
      "Naive Bayes best accuracy: 0.9085593458061554\n",
      "Random Forest Accuracy: 0.9059120093075366\n",
      "XGBoost best accuracy: 0.9149366235984854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "rf_acc=[]\n",
    "xgb_acc=[]\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_tfidf_bi, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_tfidf_bi[train_index], X_tfidf_bi[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "        \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "        \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "    # Fit and predict using Random Forest Classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and print classification report\n",
    "    rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "    print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "\n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"XGBoost Classifier bi classification report:\\n\", classification_report(y_test, xgb_pred))\n",
    "\n",
    "\n",
    "\n",
    "# Print best accuracy score for each classifier\n",
    "print(\"LinearSVC best accuracy:\", np.mean(svm_acc))\n",
    "print(\"Logistic Regression best accuracy:\", np.mean(lr_acc))\n",
    "print(\"Naive Bayes best accuracy:\", np.mean(nb_acc))\n",
    "print(\"Random Forest Accuracy:\", np.mean(rf_acc))\n",
    "print(\"XGBoost best accuracy:\", np.mean(xgb_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": 27,
>>>>>>> d413ecf (Final Commit)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.93     23208\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.93     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90     23207\n",
      "           1       0.88      0.95      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23207\n",
      "           1       0.89      0.92      0.90     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23207\n",
      "           1       0.93      0.89      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.92      0.91      0.91     46415\n",
      "weighted avg       0.92      0.91      0.91     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.91     23207\n",
      "           1       0.88      0.95      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23207\n",
      "           1       0.89      0.92      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23207\n",
      "           1       0.93      0.90      0.91     23208\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     23208\n",
      "           1       0.94      0.92      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     23208\n",
      "           1       0.94      0.93      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90     23208\n",
      "           1       0.88      0.94      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90     23208\n",
      "           1       0.89      0.92      0.90     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92     23208\n",
      "           1       0.93      0.89      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23208\n",
      "           1       0.94      0.93      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23208\n",
      "           1       0.94      0.93      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.90     23208\n",
      "           1       0.88      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23208\n",
      "           1       0.89      0.92      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23208\n",
      "           1       0.93      0.89      0.91     23207\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46414\n",
      "   macro avg       0.94      0.94      0.94     46414\n",
      "weighted avg       0.94      0.94      0.94     46414\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46414\n",
      "   macro avg       0.94      0.94      0.94     46414\n",
      "weighted avg       0.94      0.94      0.94     46414\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.91     23207\n",
      "           1       0.88      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46414\n",
      "   macro avg       0.91      0.91      0.91     46414\n",
      "weighted avg       0.91      0.91      0.91     46414\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     23207\n",
      "           1       0.89      0.92      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46414\n",
      "   macro avg       0.91      0.91      0.91     46414\n",
      "weighted avg       0.91      0.91      0.91     46414\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23207\n",
      "           1       0.93      0.90      0.92     23207\n",
      "\n",
      "    accuracy                           0.92     46414\n",
      "   macro avg       0.92      0.92      0.92     46414\n",
      "weighted avg       0.92      0.92      0.92     46414\n",
      "\n",
      "LinearSVC best accuracy: 0.9349647124611339\n",
      "Logistic Regression best accuracy: 0.9354774804232644\n",
      "Naive Bayes best accuracy: 0.9081155205165603\n",
      "Random Forest Accuracy: 0.9070754513724307\n",
      "XGBoost Classifier best accuracy: 0.9153115035232625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Set up Stratified KFold\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression(C= 10,max_iter=10000, solver='saga')\n",
    "nb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "rf_acc = []\n",
    "xgb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_tfidf_tri , y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_tfidf_tri[train_index], X_tfidf_tri[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "    # Fit and predict using Random Forest Classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and print classification report\n",
    "    rf_acc = accuracy_score(y_test, rf_pred)\n",
    "   \n",
    "    print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "\n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"XGBoost Classifier classification report:\\n\", classification_report(y_test, xgb_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print best accuracy score for each classifier\n",
    "print(\"LinearSVC best accuracy:\", np.mean(svm_acc))\n",
    "print(\"Logistic Regression best accuracy:\", np.mean(lr_acc))\n",
    "print(\"Naive Bayes best accuracy:\", np.mean(nb_acc))\n",
    "print(\"Random Forest Accuracy:\", np.mean(rf_acc))\n",
    "print(\"XGBoost Classifier best accuracy:\", np.mean(xgb_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": 28,
>>>>>>> d413ecf (Final Commit)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     23207\n",
      "           1       0.87      0.95      0.91     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "Random Forest classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     23207\n",
      "           1       0.89      0.92      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92     23207\n",
      "           1       0.93      0.90      0.91     23208\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     23207\n",
      "           1       0.94      0.93      0.94     23208\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90     23207\n",
      "           1       0.87      0.95      0.91     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "Random Forest classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     23207\n",
      "           1       0.90      0.93      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23207\n",
      "           1       0.94      0.90      0.92     23208\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23208\n",
      "           1       0.94      0.93      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     23208\n",
      "           1       0.94      0.92      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90     23208\n",
      "           1       0.87      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "Random Forest classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23208\n",
      "           1       0.89      0.92      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23208\n",
      "           1       0.93      0.89      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     23208\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23208\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     23208\n",
      "           1       0.87      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "Random Forest classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     23208\n",
      "           1       0.89      0.92      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23208\n",
      "           1       0.94      0.90      0.92     23207\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     23207\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46414\n",
      "   macro avg       0.94      0.94      0.94     46414\n",
      "weighted avg       0.94      0.94      0.94     46414\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     23207\n",
      "           1       0.94      0.93      0.94     23207\n",
      "\n",
      "    accuracy                           0.94     46414\n",
      "   macro avg       0.94      0.94      0.94     46414\n",
      "weighted avg       0.94      0.94      0.94     46414\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90     23207\n",
      "           1       0.87      0.95      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46414\n",
      "   macro avg       0.91      0.90      0.90     46414\n",
      "weighted avg       0.91      0.90      0.90     46414\n",
      "\n",
      "Random Forest classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     23207\n",
      "           1       0.90      0.93      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46414\n",
      "   macro avg       0.91      0.91      0.91     46414\n",
      "weighted avg       0.91      0.91      0.91     46414\n",
      "\n",
      "XGBoost Classifier classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     23207\n",
      "           1       0.93      0.90      0.92     23207\n",
      "\n",
      "    accuracy                           0.92     46414\n",
      "   macro avg       0.92      0.92      0.92     46414\n",
      "weighted avg       0.92      0.92      0.92     46414\n",
      "\n",
      "LinearSVC best accuracy: 0.9369209838411938\n",
      "Logistic Regression best accuracy: 0.9363478937096679\n",
      "Naive Bayes best accuracy: 0.9039961425903595\n",
      "Random Forest Accuracy: 0.9096393329598828\n",
      "XGBoost Classifier best accuracy: 0.9162508550260094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Set up Stratified KFold\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "rf_acc = []\n",
    "xgb_acc=[]\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_tfidf_uni , y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_tfidf_uni[train_index], X_tfidf_uni[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "\n",
    "\n",
    "    # Fit and predict using Random Forest Classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    # Calculate accuracy and print classification report\n",
    "    rf_acc = accuracy_score(y_test, rf_pred)\n",
    "    # Print classification report\n",
    "    print(\"Random Forest classification report:\\n\", classification_report(y_test, rf_pred))\n",
    "\n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"XGBoost Classifier classification report:\\n\", classification_report(y_test, xgb_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print best accuracy score for each classifier\n",
    "print(\"LinearSVC best accuracy:\", np.mean(svm_acc))\n",
    "print(\"Logistic Regression best accuracy:\", np.mean(lr_acc))\n",
    "print(\"Naive Bayes best accuracy:\", np.mean(nb_acc))\n",
    "print(\"Random Forest Accuracy:\", np.mean(rf_acc))\n",
    "print(\"XGBoost Classifier best accuracy:\", np.mean(xgb_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     23207\n",
      "           1       0.94      0.90      0.92     23208\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.93      0.92      0.92     46415\n",
      "weighted avg       0.93      0.92      0.92     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     23207\n",
      "           1       0.95      0.91      0.93     23208\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89     23207\n",
      "           1       0.86      0.96      0.91     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90     23207\n",
      "           1       0.88      0.92      0.90     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     23207\n",
      "           1       0.93      0.89      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
<<<<<<< HEAD
      "LinearSVC best accuracy: 0.9244425293547345\n",
=======
      "LinearSVC best accuracy: 0.9244209845954972\n",
>>>>>>> d413ecf (Final Commit)
      "Logistic Regression best accuracy: 0.930151890552623\n",
      "Naive Bayes best accuracy: 0.9003985780458903\n",
      "Random Forest Accuracy: 0.8989766239362275\n",
      "XGBoost best accuracy: 0.9112786814607347\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     23207\n",
      "           1       0.95      0.90      0.92     23208\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     23207\n",
      "           1       0.95      0.91      0.93     23208\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90     23207\n",
      "           1       0.87      0.96      0.91     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90     23207\n",
      "           1       0.89      0.92      0.91     23208\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92     23207\n",
      "           1       0.94      0.89      0.91     23208\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.92      0.91      0.91     46415\n",
      "weighted avg       0.92      0.91      0.91     46415\n",
      "\n",
<<<<<<< HEAD
      "LinearSVC best accuracy: 0.9250565549929979\n",
=======
      "LinearSVC best accuracy: 0.9250134654745233\n",
>>>>>>> d413ecf (Final Commit)
      "Logistic Regression best accuracy: 0.9307120542927932\n",
      "Naive Bayes best accuracy: 0.9024022406549607\n",
      "Random Forest Accuracy: 0.9031993967467413\n",
      "XGBoost best accuracy: 0.912625228913067\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     23208\n",
      "           1       0.95      0.90      0.92     23207\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     23208\n",
      "           1       0.95      0.91      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89     23208\n",
      "           1       0.86      0.96      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90     23208\n",
      "           1       0.88      0.92      0.90     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     23208\n",
      "           1       0.93      0.88      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
<<<<<<< HEAD
      "LinearSVC best accuracy: 0.924270171280836\n",
=======
      "LinearSVC best accuracy: 0.9241768106574743\n",
>>>>>>> d413ecf (Final Commit)
      "Logistic Regression best accuracy: 0.9298646270961255\n",
      "Naive Bayes best accuracy: 0.9018061689827283\n",
      "Random Forest Accuracy: 0.8976623936227512\n",
      "XGBoost best accuracy: 0.9117957556824302\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     23208\n",
      "           1       0.95      0.90      0.92     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     23208\n",
      "           1       0.95      0.91      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90     23208\n",
      "           1       0.86      0.96      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.91      0.90      0.90     46415\n",
      "weighted avg       0.91      0.90      0.90     46415\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90     23208\n",
      "           1       0.89      0.92      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46415\n",
      "   macro avg       0.90      0.90      0.90     46415\n",
      "weighted avg       0.90      0.90      0.90     46415\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92     23208\n",
      "           1       0.94      0.89      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.91      0.91      0.91     46415\n",
      "weighted avg       0.91      0.91      0.91     46415\n",
      "\n",
<<<<<<< HEAD
      "LinearSVC best accuracy: 0.9246041150490143\n",
=======
      "LinearSVC best accuracy: 0.9245610255305398\n",
>>>>>>> d413ecf (Final Commit)
      "Logistic Regression best accuracy: 0.9301788215016698\n",
      "Naive Bayes best accuracy: 0.9021006140256382\n",
      "Random Forest Accuracy: 0.9033071205429279\n",
      "XGBoost best accuracy: 0.9121674027792739\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     23207\n",
      "           1       0.95      0.90      0.92     23207\n",
      "\n",
      "    accuracy                           0.93     46414\n",
      "   macro avg       0.93      0.93      0.93     46414\n",
      "weighted avg       0.93      0.93      0.93     46414\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     23207\n",
      "           1       0.95      0.91      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46414\n",
      "   macro avg       0.93      0.93      0.93     46414\n",
      "weighted avg       0.93      0.93      0.93     46414\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90     23207\n",
      "           1       0.86      0.96      0.91     23207\n",
      "\n",
      "    accuracy                           0.90     46414\n",
      "   macro avg       0.91      0.90      0.90     46414\n",
      "weighted avg       0.91      0.90      0.90     46414\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90     23207\n",
      "           1       0.89      0.92      0.90     23207\n",
      "\n",
      "    accuracy                           0.90     46414\n",
      "   macro avg       0.90      0.90      0.90     46414\n",
      "weighted avg       0.90      0.90      0.90     46414\n",
      "\n",
      "XGBoost Classifier bi classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92     23207\n",
      "           1       0.94      0.89      0.91     23207\n",
      "\n",
      "    accuracy                           0.91     46414\n",
      "   macro avg       0.91      0.91      0.91     46414\n",
      "weighted avg       0.91      0.91      0.91     46414\n",
      "\n",
<<<<<<< HEAD
      "LinearSVC best accuracy: 0.9248170878766743\n",
=======
      "LinearSVC best accuracy: 0.9247826162618946\n",
>>>>>>> d413ecf (Final Commit)
      "Logistic Regression best accuracy: 0.9305738754889215\n",
      "Naive Bayes best accuracy: 0.9022122273346141\n",
      "Random Forest Accuracy: 0.9018184168569828\n",
      "XGBoost best accuracy: 0.912424489724604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "rf_acc=[]\n",
    "xgb_acc=[]\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_bow, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_bow[train_index], X_bow[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "        \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "        \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "    # Fit and predict using Random Forest Classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and print classification report\n",
    "    rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "    print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "\n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"XGBoost Classifier bi classification report:\\n\", classification_report(y_test, xgb_pred))\n",
    "\n",
    "\n",
    "\n",
    "    # Print best accuracy score for each classifier\n",
    "    print(\"LinearSVC best accuracy:\", np.mean(svm_acc))\n",
    "    print(\"Logistic Regression best accuracy:\", np.mean(lr_acc))\n",
    "    print(\"Naive Bayes best accuracy:\", np.mean(nb_acc))\n",
    "    print(\"Random Forest Accuracy:\", np.mean(rf_acc))\n",
    "    print(\"XGBoost best accuracy:\", np.mean(xgb_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 31,
>>>>>>> d413ecf (Final Commit)
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m xgb_acc\u001b[39m=\u001b[39m[]\n\u001b[1;32m     26\u001b[0m \u001b[39m# Perform stratified k-fold cross-validation on selected features\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m skf\u001b[39m.\u001b[39msplit(word2vec, y):\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Split data into training and testing sets\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m word2vec[train_index], word2vec[test_index]\n\u001b[1;32m     30\u001b[0m     y_train, y_test \u001b[39m=\u001b[39m y[train_index], y[test_index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Set up Stratified KFold\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Define classifiers\n",
    "# svm = LinearSVC()\n",
    "# lr = LogisticRegression()\n",
    "# nb = GaussianNB()\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# # Define lists to store accuracy scores\n",
    "# svm_acc = []\n",
    "# lr_acc = []\n",
    "# nb_acc = []\n",
    "# rf_acc=[]\n",
    "# xgb_acc=[]\n",
    "\n",
    "# # Perform stratified k-fold cross-validation on selected features\n",
    "# for train_index, test_index in skf.split(word2vec, y):\n",
    "#     # Split data into training and testing sets\n",
    "#     X_train, X_test = word2vec[train_index], word2vec[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     # Fit and predict using LinearSVC\n",
    "#     svm.fit(X_train, y_train)\n",
    "#     svm_pred = svm.predict(X_test)\n",
    "#     svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "#     print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "        \n",
    "#     # Fit and predict using Logistic Regression\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     lr_pred = lr.predict(X_test)\n",
    "#     lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "#     print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "        \n",
    "#     # Fit and predict using Naive Bayes\n",
    "#     nb.fit(X_train, y_train)\n",
    "#     nb_pred = nb.predict(X_test)\n",
    "#     nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "#     print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "#     # Fit and predict using Random Forest Classifier\n",
    "#     rf.fit(X_train, y_train)\n",
    "#     rf_pred = rf.predict(X_test)\n",
    "\n",
    "#     # Calculate accuracy and print classification report\n",
    "#     rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "#     print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "\n",
    "#     # Fit and predict using XGBoost Classifier\n",
    "#     xgb.fit(X_train, y_train, eval_metric='error')\n",
    "#     xgb_pred = xgb.predict(X_test)\n",
    "#     xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "#     # Print classification report\n",
    "#     print(\"XGBoost Classifier bi classification report:\\n\", classification_report(y_test, xgb_pred))\n",
    "\n",
    "\n",
    "\n",
    "#     # Print best accuracy score for each classifier\n",
    "#     print(\"LinearSVC best accuracy:\", np.mean(svm_acc))\n",
    "#     print(\"Logistic Regression best accuracy:\", np.mean(lr_acc))\n",
    "#     print(\"Naive Bayes best accuracy:\", np.mean(nb_acc))\n",
    "#     print(\"Random Forest Accuracy:\", np.mean(rf_acc))\n",
    "#     print(\"XGBoost best accuracy:\", np.mean(xgb_acc))\n",
=======
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "nb = GaussianNB()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "rf_acc=[]\n",
    "xgb_acc=[]\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(word2vec, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = word2vec[train_index], word2vec[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "        \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "        \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "    # Fit and predict using Random Forest Classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and print classification report\n",
    "    rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "    print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "\n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"XGBoost Classifier bi classification report:\\n\", classification_report(y_test, xgb_pred))\n",
    "\n",
    "\n",
    "\n",
    "    # Print best accuracy score for each classifier\n",
    "    print(\"LinearSVC best accuracy:\", np.mean(svm_acc))\n",
    "    print(\"Logistic Regression best accuracy:\", np.mean(lr_acc))\n",
    "    print(\"Naive Bayes best accuracy:\", np.mean(nb_acc))\n",
    "    print(\"Random Forest Accuracy:\", np.mean(rf_acc))\n",
    "    print(\"XGBoost best accuracy:\", np.mean(xgb_acc))\n",
>>>>>>> d413ecf (Final Commit)
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Create a figure to plot the ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_uni, y, test_size=0.2, random_state=42)\n",
    "# Define the classifiers\n",
    "classifiers = {'LinearSVC': svm, 'Logistic Regression': lr, 'Naive Bayes': nb, 'Random Forest': rf,'XGBClassifier':xgb}\n",
    "\n",
    "# Loop through each classifier and plot its ROC curve\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier and predict the probabilities of the positive class\n",
    "    clf.fit(X_train, y_train)\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute the ROC curve and its AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the ROC curve and its AUC\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the random classifier curve\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "\n",
    "# Set the labels and legend\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC curves')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Create a figure to plot the ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_bi, y, test_size=0.2, random_state=42)\n",
    "# Define the classifiers\n",
    "classifiers = {'LinearSVC': svm, 'Logistic Regression': lr, 'Naive Bayes': nb, 'Random Forest': rf,'XGBClassifier':xgb}\n",
    "\n",
    "# Loop through each classifier and plot its ROC curve\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier and predict the probabilities of the positive class\n",
    "    clf.fit(X_train, y_train)\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute the ROC curve and its AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the ROC curve and its AUC\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the random classifier curve\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "\n",
    "# Set the labels and legend\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC curves')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Create a figure to plot the ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_tri, y, test_size=0.2, random_state=42)\n",
    "# Define the classifiers\n",
    "classifiers = {'LinearSVC': svm, 'Logistic Regression': lr, 'Naive Bayes': nb, 'Random Forest': rf,'XGBClassifier':xgb}\n",
    "\n",
    "# Loop through each classifier and plot its ROC curve\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier and predict the probabilities of the positive class\n",
    "    clf.fit(X_train, y_train)\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute the ROC curve and its AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the ROC curve and its AUC\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the random classifier curve\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "\n",
    "# Set the labels and legend\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC curves')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Create a figure to plot the ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "# Define the classifiers\n",
    "classifiers = {'LinearSVC': svm, 'Logistic Regression': lr, 'Naive Bayes': nb, 'Random Forest': rf,'XGBClassifier':xgb}\n",
    "\n",
    "# Loop through each classifier and plot its ROC curve\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier and predict the probabilities of the positive class\n",
    "    clf.fit(X_train, y_train)\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute the ROC curve and its AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the ROC curve and its AUC\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the random classifier curve\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "\n",
    "# Set the labels and legend\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC curves')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a figure to plot the ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)\n",
    "# Define the classifiers\n",
    "# classifiers = {'LinearSVC': svm, 'Logistic Regression': lr, 'Naive Bayes': nb, 'Random Forest': rf,'XGBClassifier':xgb}\n",
    "classifiers = {'LinearSVC': svm, 'Logistic Regression': lr, 'Naive Bayes': GaussianNB(), 'Random Forest': rf,'XGBClassifier':xgb}\n",
    "\n",
    "# Loop through each classifier and plot its ROC curve\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier and predict the probabilities of the positive class\n",
    "    clf.fit(X_train, y_train)\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute the ROC curve and its AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the ROC curve and its AUC\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the random classifier curve\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "\n",
    "# Set the labels and legend\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC curves')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
