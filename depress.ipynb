{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/prashantkarna/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "nltk.download('vader_lexicon')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data1 = pd.read_csv('/Users/prashantkarna/Documents/Research Materials/Datasets/Suicide_Detection.csv')\n",
    "data1['class'] = data1['class'].replace(['suicide'], 1)\n",
    "data1['class'] = data1['class'].replace(['non-suicide'], 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data2 = pd.read_csv('/Users/prashantkarna/Documents/Research Materials/Datasets/depression_dataset_reddit_cleaned.csv')\n",
    "# rename the columns\n",
    "data2 = data2.rename(columns={'clean_text': 'text', 'is_depression': 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239805, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data1,data2])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    119937\n",
      "1    119868\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of instances in each class\n",
    "class_counts = data['class'].value_counts()\n",
    "\n",
    "# Print the class counts\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_split = np.array_split(data, 3)\n",
    "data = data_split[0]\n",
    "data = data.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashantkarna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove emails\n",
    "email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    # removing all special charachter\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(X['text'][i]))\n",
    "    # make document as lowerCase\n",
    "    review = review.lower()\n",
    "    # splitting the documents into words for ex ['iam', 'omar']\n",
    "    review = review.split()\n",
    "    # make limmatization --> (change, changing, changes)---> (change)\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords)]\n",
    "    # join the document agian\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    # removing mails\n",
    "    for r in regexes_to_remove:\n",
    "        X['text'][i] = re.sub(r, '', review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_vectorizer_uni = TfidfVectorizer(max_features=10000,ngram_range=(1,1))\n",
    "tfidf_vectorizer_bi = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "tfidf_vectorizer_tri = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X['text'])\n",
    "# X_tfidf_test = tfidf_vectorizer.transform(X['text'])\n",
    "\n",
    "X_tfidf_uni = tfidf_vectorizer_uni.fit_transform(X['text'])\n",
    "# X_tfidf_test_uni = tfidf_vectorizer_uni.transform(X['text'])\n",
    "\n",
    "X_tfidf_bi = tfidf_vectorizer_bi.fit_transform(X['text'])\n",
    "# X_tfidf_test_bi =tfidf_vectorizer_bi.transform(X'text'])\n",
    "\n",
    "X_tfidf_tri = tfidf_vectorizer_tri.fit_transform(X['text'])\n",
    "# X_tfidf_test_tri = tfidf_vectorizer_tri.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA feature extraction on unigrams\n",
    "lda_uni = LatentDirichletAllocation(n_components=3)\n",
    "lda_uni.fit(X_tfidf_uni)\n",
    "lda_features_uni = lda_uni.transform(X_tfidf_uni)\n",
    "data_lda_uni = pd.DataFrame(lda_features_uni, columns=['topic_1', 'topic_2', 'topic_3'])\n",
    "\n",
    "# LDA feature extraction on bigrams\n",
    "lda_bi = LatentDirichletAllocation(n_components=3)\n",
    "lda_bi.fit(X_tfidf_bi)\n",
    "lda_features_bi = lda_bi.transform(X_tfidf_bi)\n",
    "data_lda_bi = pd.DataFrame(lda_features_bi, columns=['topic_1', 'topic_2', 'topic_3'])\n",
    "\n",
    "# LDA feature extraction on trigrams\n",
    "lda_tri = LatentDirichletAllocation(n_components=3)\n",
    "lda_tri.fit(X_tfidf_tri)\n",
    "lda_features_tri = lda_tri.transform(X_tfidf_tri)\n",
    "data_lda_tri = pd.DataFrame(lda_features_tri, columns=['topic_1', 'topic_2', 'topic_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LDA feature extraction on unigrams\n",
    "# lda_uni = LatentDirichletAllocation(n_components=3)\n",
    "# lda_uni.fit(X_tfidf_train_uni)\n",
    "# lda_features_uni = lda_uni.transform(X_tfidf_train_uni)\n",
    "# data_lda_uni = pd.DataFrame(lda_features_uni, columns=['topic_1', 'topic_2', 'topic_3'])\n",
    "\n",
    "# # LDA feature extraction on bigrams\n",
    "# lda_bi = LatentDirichletAllocation(n_components=3)\n",
    "# lda_bi.fit(X_tfidf_train_bi)\n",
    "# lda_features_bi = lda_bi.transform(X_tfidf_train_bi)\n",
    "# data_lda_bi = pd.DataFrame(lda_features_bi, columns=['topic_1', 'topic_2', 'topic_3'])\n",
    "\n",
    "# # LDA feature extraction on trigrams\n",
    "# lda_tri = LatentDirichletAllocation(n_components=3)\n",
    "# lda_tri.fit(X_tfidf_train_tri)\n",
    "# lda_features_tri = lda_tri.transform(X_tfidf_train_tri)\n",
    "# data_lda_tri = pd.DataFrame(lda_features_tri, columns=['topic_1', 'topic_2', 'topic_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Feature selection using ExtraTreesClassifier and SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(ExtraTreesClassifier(n_estimators=300, random_state=42))\n",
    "\n",
    "                                        \n",
    "# Unigram feature selection\n",
    "selector_tfidf= selector.fit(X_tfidf, y)\n",
    "X_selection = selector_tfidf.transform(X_tfidf)\n",
    "\n",
    "\n",
    "# Unigram feature selection\n",
    "selector_uni= selector.fit(X_tfidf_uni, y)\n",
    "X_selection_uni = selector_uni.transform(X_tfidf_uni)\n",
    "\n",
    "# Bigram feature selection\n",
    "selector_bi = selector.fit(X_tfidf_bi, y)\n",
    "X_selection_bi = selector_bi.transform(X_tfidf_bi)\n",
    "\n",
    "# Trigram feature selection\n",
    "selector_tri= selector.fit(X_tfidf_tri, y)\n",
    "X_selection_tri = selector_tri.transform(X_tfidf_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection using LDA and ExtraTreesClassifier on unigrams\n",
    "selector_lda_uni = SelectFromModel(ExtraTreesClassifier(n_estimators=300, random_state=42))\n",
    "X_selection_lda_uni = selector_lda_uni.fit_transform(lda_features_uni, y)\n",
    "\n",
    "# Perform feature selection using LDA and ExtraTreesClassifier on bigrams\n",
    "selector_lda_bi = SelectFromModel(ExtraTreesClassifier(n_estimators=300, random_state=42))\n",
    "X_selection_lda_bi = selector_lda_bi.fit_transform(lda_features_bi, y)\n",
    "\n",
    "# Perform feature selection using LDA and ExtraTreesClassifier on trigrams\n",
    "selector_lda_tri = SelectFromModel(ExtraTreesClassifier(n_estimators=300, random_state=42))\n",
    "X_selection_lda_tri = selector_lda_tri.fit_transform(lda_features_tri, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier (LDA features for unigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      8035\n",
      "           1       0.86      0.87      0.86      7952\n",
      "\n",
      "    accuracy                           0.86     15987\n",
      "   macro avg       0.86      0.86      0.86     15987\n",
      "weighted avg       0.86      0.86      0.86     15987\n",
      "\n",
      "SVM Classifier (LDA features for unigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87      8035\n",
      "           1       0.86      0.90      0.88      7952\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.88      0.88      0.88     15987\n",
      "weighted avg       0.88      0.88      0.88     15987\n",
      "\n",
      "Logistic Regression (LDA features for unigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87      8035\n",
      "           1       0.85      0.91      0.88      7952\n",
      "\n",
      "    accuracy                           0.87     15987\n",
      "   macro avg       0.88      0.87      0.87     15987\n",
      "weighted avg       0.88      0.87      0.87     15987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets for unigrams\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selection_lda_uni, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier on LDA features for unigrams\n",
    "rf_uni = RandomForestClassifier(random_state=42)\n",
    "rf_uni.fit(X_train, y_train)\n",
    "y_pred_rf_uni = rf_uni.predict(X_test)\n",
    "print(\"Random Forest Classifier (LDA features for unigrams):\\n\", classification_report(y_test, y_pred_rf_uni))\n",
    "\n",
    "# SVM Classifier on LDA features for unigrams\n",
    "svm_uni = SVC(random_state=42)\n",
    "svm_uni.fit(X_train, y_train)\n",
    "y_pred_svm_uni = svm_uni.predict(X_test)\n",
    "print(\"SVM Classifier (LDA features for unigrams):\\n\", classification_report(y_test, y_pred_svm_uni))\n",
    "\n",
    "# Logistic Regression on LDA features for unigrams\n",
    "lr_uni = LogisticRegression(random_state=42)\n",
    "lr_uni.fit(X_train, y_train)\n",
    "y_pred_lr_uni = lr_uni.predict(X_test)\n",
    "print(\"Logistic Regression (LDA features for unigrams):\\n\", classification_report(y_test, y_pred_lr_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier (LDA features for bigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88      8035\n",
      "           1       0.86      0.90      0.88      7952\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.88      0.88      0.88     15987\n",
      "weighted avg       0.88      0.88      0.88     15987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Classifier on LDA features for unigram\n",
    "xgb_uni = XGBClassifier(random_state=42)\n",
    "xgb_uni.fit(X_train, y_train)\n",
    "y_pred_xgb_uni = xgb_uni.predict(X_test)\n",
    "print(\"XGBoost Classifier (LDA features for bigrams):\\n\", classification_report(y_test, y_pred_xgb_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier (LDA features for bigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      8035\n",
      "           1       0.88      0.88      0.88      7952\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.88      0.88      0.88     15987\n",
      "weighted avg       0.88      0.88      0.88     15987\n",
      "\n",
      "SVM Classifier (LDA features for bigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88      8035\n",
      "           1       0.87      0.91      0.89      7952\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "Logistic Regression (LDA features for bigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88      8035\n",
      "           1       0.87      0.91      0.89      7952\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.88      0.88      0.88     15987\n",
      "weighted avg       0.88      0.88      0.88     15987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets for bigrams\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selection_lda_bi, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier on LDA features for bigrams\n",
    "rf_bi = RandomForestClassifier(random_state=42)\n",
    "rf_bi.fit(X_train, y_train)\n",
    "y_pred_rf_bi = rf_bi.predict(X_test)\n",
    "print(\"Random Forest Classifier (LDA features for bigrams):\\n\", classification_report(y_test, y_pred_rf_bi))\n",
    "\n",
    "# SVM Classifier on LDA features for bigrams\n",
    "svm_bi = SVC(random_state=42)\n",
    "svm_bi.fit(X_train, y_train)\n",
    "y_pred_svm_bi = svm_bi.predict(X_test)\n",
    "print(\"SVM Classifier (LDA features for bigrams):\\n\", classification_report(y_test, y_pred_svm_bi))\n",
    "\n",
    "# Logistic Regression on LDA features for bigrams\n",
    "lr_bi = LogisticRegression(random_state=42)\n",
    "lr_bi.fit(X_train, y_train)\n",
    "y_pred_lr_bi = lr_bi.predict(X_test)\n",
    "print(\"Logistic Regression (LDA features for bigrams):\\n\", classification_report(y_test, y_pred_lr_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier (LDA features for bigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      8035\n",
      "           1       0.89      0.90      0.89      7952\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Classifier on LDA features for bigrams\n",
    "xgb_bi = XGBClassifier(random_state=42)\n",
    "xgb_bi.fit(X_train, y_train)\n",
    "y_pred_xgb_bi = xgb_bi.predict(X_test)\n",
    "print(\"XGBoost Classifier (LDA features for bigrams):\\n\", classification_report(y_test, y_pred_xgb_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier (LDA features for trigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      8035\n",
      "           1       0.80      0.80      0.80      7952\n",
      "\n",
      "    accuracy                           0.80     15987\n",
      "   macro avg       0.80      0.80      0.80     15987\n",
      "weighted avg       0.80      0.80      0.80     15987\n",
      "\n",
      "SVM Classifier (LDA features for trigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      8035\n",
      "           1       0.84      0.89      0.86      7952\n",
      "\n",
      "    accuracy                           0.86     15987\n",
      "   macro avg       0.86      0.86      0.86     15987\n",
      "weighted avg       0.86      0.86      0.86     15987\n",
      "\n",
      "Logistic Regression (LDA features for trigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84      8035\n",
      "           1       0.80      0.92      0.86      7952\n",
      "\n",
      "    accuracy                           0.85     15987\n",
      "   macro avg       0.86      0.85      0.85     15987\n",
      "weighted avg       0.86      0.85      0.85     15987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets for trigrams\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selection_lda_tri , y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier on LDA features for trigrams\n",
    "rf_tri = RandomForestClassifier(random_state=42)\n",
    "rf_tri.fit(X_train, y_train)\n",
    "y_pred_rf_tri = rf_tri.predict(X_test)\n",
    "print(\"Random Forest Classifier (LDA features for trigrams):\\n\", classification_report(y_test, y_pred_rf_tri))\n",
    "\n",
    "# SVM Classifier on LDA features for trigrams\n",
    "svm_tri = SVC(random_state=42)\n",
    "svm_tri.fit(X_train, y_train)\n",
    "y_pred_svm_tri = svm_tri.predict(X_test)\n",
    "print(\"SVM Classifier (LDA features for trigrams):\\n\", classification_report(y_test, y_pred_svm_tri))\n",
    "\n",
    "# Logistic Regression on LDA features for trigrams\n",
    "lr_tri = LogisticRegression(random_state=42)\n",
    "lr_tri.fit(X_train, y_train)\n",
    "y_pred_lr_tri = lr_tri.predict(X_test)\n",
    "print(\"Logistic Regression (LDA features for trigrams):\\n\", classification_report(y_test, y_pred_lr_tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier (LDA features for trigrams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85      8035\n",
      "           1       0.84      0.88      0.86      7952\n",
      "\n",
      "    accuracy                           0.85     15987\n",
      "   macro avg       0.86      0.86      0.85     15987\n",
      "weighted avg       0.86      0.85      0.85     15987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Classifier on LDA features for trigrams\n",
    "xgb_tri = XGBClassifier(random_state=42)\n",
    "xgb_tri.fit(X_train, y_train,eval_metric='error')\n",
    "y_pred_xgb_tri = xgb_tri.predict(X_test)\n",
    "print(\"XGBoost Classifier (LDA features for trigrams):\\n\", classification_report(y_test, y_pred_xgb_tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to csv\n",
    "# X_df = pd.DataFrame(X_selection.todense())\n",
    "# X_df.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to cs\n",
    "# X_df_uni = pd.DataFrame(X_selection.todense())\n",
    "# X_df_uni.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_uni_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to csv\n",
    "# X_df_bi = pd.DataFrame(X_selection.todense())\n",
    "# X_df_bi.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_bi_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted features to csv\n",
    "# X_df_tri = pd.DataFrame(X_selection.todense())\n",
    "# X_df_tri.to_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_tri_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '920', '921', '922', '923', '924', '925', '926', '927', '928', '929'],\n",
       "      dtype='object', length=930)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data1 = pd.read_csv('/Users/prashantkarna/Documents/Research Materials/final code/feature_tri_dataset.csv')\n",
    "# data1.columns\n",
    "\n",
    "# # X = data1.drop('class', axis=1)\n",
    "# # y = data1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/prashantkarna/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in /Users/prashantkarna/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from xgboost) (1.24.2)\n",
      "Requirement already satisfied: scipy in /Users/prashantkarna/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from xgboost) (1.10.1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      7999\n",
      "           1       0.93      0.92      0.92      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      7999\n",
      "           1       0.93      0.92      0.92      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88      7999\n",
      "           1       0.85      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      7999\n",
      "           1       0.85      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88      7999\n",
      "           1       0.85      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      8000\n",
      "           1       0.94      0.92      0.93      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      8000\n",
      "           1       0.93      0.92      0.93      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88      8000\n",
      "           1       0.85      0.94      0.89      7987\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      8000\n",
      "           1       0.94      0.91      0.93      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      8000\n",
      "           1       0.94      0.91      0.92      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      8000\n",
      "           1       0.85      0.94      0.89      7987\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression(C=500, penalty='l1', solver='saga')\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_bi, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_bi[train_index], X_selection_bi[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "# # Print best accuracy score for each classifier\n",
    "# print(\"LinearSVC best accuracy:\", max(svm_acc))\n",
    "# print(\"Logistic Regression best accuracy:\", max(lr_acc))\n",
    "# print(\"Naive Bayes best accuracy:\", max(nb_acc))\n",
    "# # print(\"XGBoost best accuracy:\", max(xgb_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92      7999\n",
      "           1       0.93      0.92      0.92      7988\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.92      0.92      0.92     15987\n",
      "weighted avg       0.92      0.92      0.92     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92      7999\n",
      "           1       0.93      0.91      0.92      7988\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.92      0.92      0.92     15987\n",
      "weighted avg       0.92      0.92      0.92     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88      7999\n",
      "           1       0.85      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.88     15987\n",
      "weighted avg       0.89      0.89      0.88     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.92      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      7999\n",
      "           1       0.85      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.92      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      7999\n",
      "           1       0.85      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      8000\n",
      "           1       0.93      0.92      0.93      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92      8000\n",
      "           1       0.93      0.91      0.92      7987\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.92      0.92      0.92     15987\n",
      "weighted avg       0.92      0.92      0.92     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88      8000\n",
      "           1       0.85      0.94      0.89      7987\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      8000\n",
      "           1       0.94      0.91      0.93      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      8000\n",
      "           1       0.94      0.91      0.92      7987\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.92      0.92      0.92     15987\n",
      "weighted avg       0.92      0.92      0.92     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      8000\n",
      "           1       0.85      0.94      0.89      7987\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC best accuracy: 0.9295677738162257\n",
      "Logistic Regression best accuracy: 0.9256896228185401\n",
      "Naive Bayes best accuracy: 0.8887846375179833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC(C=1.0,dual=False, max_iter=1000,random_state=42)\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_tri, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_tri[train_index], X_selection_tri[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "# Print best accuracy score for each classifier\n",
    "print(\"LinearSVC best accuracy:\", max(svm_acc))\n",
    "print(\"Logistic Regression best accuracy:\", max(lr_acc))\n",
    "print(\"Naive Bayes best accuracy:\", max(nb_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      7999\n",
      "           1       0.93      0.92      0.92      7988\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.93      0.92      0.92     15987\n",
      "weighted avg       0.93      0.92      0.92     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      7999\n",
      "           1       0.93      0.91      0.92      7988\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.92      0.92      0.92     15987\n",
      "weighted avg       0.92      0.92      0.92     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88      7999\n",
      "           1       0.84      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.89      0.88      0.88     15987\n",
      "weighted avg       0.89      0.88      0.88     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.92      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      7999\n",
      "           1       0.85      0.93      0.89      7988\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.89      0.89      0.88     15987\n",
      "weighted avg       0.89      0.88      0.88     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.93      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      7999\n",
      "           1       0.94      0.91      0.92      7988\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.87      7999\n",
      "           1       0.84      0.94      0.89      7988\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.89      0.88      0.88     15987\n",
      "weighted avg       0.89      0.88      0.88     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      8000\n",
      "           1       0.94      0.92      0.93      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      8000\n",
      "           1       0.93      0.91      0.92      7987\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.92      0.92      0.92     15987\n",
      "weighted avg       0.92      0.92      0.92     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88      8000\n",
      "           1       0.85      0.94      0.89      7987\n",
      "\n",
      "    accuracy                           0.88     15987\n",
      "   macro avg       0.89      0.88      0.88     15987\n",
      "weighted avg       0.89      0.88      0.88     15987\n",
      "\n",
      "LinearSVC classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      8000\n",
      "           1       0.94      0.91      0.92      7987\n",
      "\n",
      "    accuracy                           0.93     15987\n",
      "   macro avg       0.93      0.93      0.93     15987\n",
      "weighted avg       0.93      0.93      0.93     15987\n",
      "\n",
      "Logistic Regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      8000\n",
      "           1       0.94      0.90      0.92      7987\n",
      "\n",
      "    accuracy                           0.92     15987\n",
      "   macro avg       0.92      0.92      0.92     15987\n",
      "weighted avg       0.92      0.92      0.92     15987\n",
      "\n",
      "Naive Bayes classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      8000\n",
      "           1       0.85      0.93      0.89      7987\n",
      "\n",
      "    accuracy                           0.89     15987\n",
      "   macro avg       0.89      0.89      0.89     15987\n",
      "weighted avg       0.89      0.89      0.89     15987\n",
      "\n",
      "LinearSVC best accuracy: 0.9301932820416589\n",
      "Logistic Regression best accuracy: 0.9259398261087133\n",
      "Naive Bayes best accuracy: 0.8857196472133608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Define lists to store accuracy scores\n",
    "svm_acc = []\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_uni, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_uni[train_index], X_selection_uni[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using LinearSVC\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_acc.append(accuracy_score(y_test, svm_pred))\n",
    "    print(\"LinearSVC classification report:\\n\", classification_report(y_test, svm_pred))\n",
    "    \n",
    "    # Fit and predict using Logistic Regression\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_acc.append(accuracy_score(y_test, lr_pred))\n",
    "    print(\"Logistic Regression classification report:\\n\", classification_report(y_test, lr_pred))\n",
    "    \n",
    "    # Fit and predict using Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_acc.append(accuracy_score(y_test, nb_pred))\n",
    "    print(\"Naive Bayes classification report:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "# Print best accuracy score for each classifier\n",
    "print(\"LinearSVC best accuracy:\", max(svm_acc))\n",
    "print(\"Logistic Regression best accuracy:\", max(lr_acc))\n",
    "print(\"Naive Bayes best accuracy:\", max(nb_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier best accuracy: 0.899793582285607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define list to store accuracy scores\n",
    "rf_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_tri, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_tri[train_index], X_selection_tri[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using Random Forest Classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_acc.append(accuracy_score(y_test, rf_pred))\n",
    "\n",
    "# Print best accuracy score for Random Forest Classifier\n",
    "print(\"Random Forest Classifier best accuracy:\", max(rf_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier tri best accuracy: 0.9139926190029399\n",
      "XGBoost Classifier bi  best accuracy: 0.9160567961468693\n",
      "XGBoost Classifier tri best accuracy: 0.9160567961468693\n",
      "XGBoost Classifier uni  best accuracy: 0.9160567961468693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define XGBoost Classifier\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define list to store accuracy scores\n",
    "xgb_acc = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_tri, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_tri[train_index], X_selection_tri[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "# Print best accuracy score for XGBoost Classifier\n",
    "print(\"XGBoost Classifier tri best accuracy:\", max(xgb_acc))\n",
    "\n",
    "#  Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_bi, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_bi[train_index], X_selection_bi[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "# Print best accuracy score for XGBoost Classifier\n",
    "print(\"XGBoost Classifier bi  best accuracy:\", max(xgb_acc))\n",
    "\n",
    "# Print best accuracy score for XGBoost Classifier\n",
    "print(\"XGBoost Classifier tri best accuracy:\", max(xgb_acc))\n",
    "\n",
    "#  Perform stratified k-fold cross-validation on selected features\n",
    "for train_index, test_index in skf.split(X_selection_uni, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_selection_uni[train_index], X_selection_uni[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit and predict using XGBoost Classifier\n",
    "    xgb.fit(X_train, y_train, eval_metric='error')\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_acc.append(accuracy_score(y_test, xgb_pred))\n",
    "\n",
    "# Print best accuracy score for XGBoost Classifier\n",
    "print(\"XGBoost Classifier uni  best accuracy:\", max(xgb_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier best accuracy: 0.8819080860965678\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9273479413095469\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
